# SupplyGraph Backend

## **Simplified Architecture (No Company Management)**

### **What We Store Where:**

#### **1. MongoDB Atlas (Cloud Database)**
- âœ… **Models Collection** - Base GCN model (shared)
- âœ… **Company Models Collection** - Fine-tuned models per company

#### **2. Local File System (uploads/)**
- âœ… **Raw CSV files** - User uploads
- âœ… **Processed CSV files** - nodes.csv, edges.csv, demand.csv

### **Key Benefits:**
- âœ… **Simple** - No unnecessary company tracking
- âœ… **Direct** - Just file processing and ML operations
- âœ… **Clean** - Minimal code, maximum functionality
- âœ… **Working** - Core ML functionality intact

## **Data Flow:**

```
User Upload â†’ Process CSV â†’ Fine-tune â†’ Store Model in Atlas
     â†“           â†“           â†“         â†“
  Raw CSV    Convert to    ML      Company Model
              Structured   Service  (with company_id)
```

## **API Endpoints:**

### **Data Processing**
- `POST /api/data/convert/:companyId` - Process raw CSV to structured format

### **ML Operations**
- `POST /api/ml/create-sample/:companyId` - Generate sample dataset
- `POST /api/ml/fine-tune/:companyId` - Fine-tune model (requires nodes, edges, demand in body)
- `POST /api/ml/predict/:companyId` - Make predictions
- `GET /api/ml/health` - ML service health check
- `GET /api/ml/training-status/:companyId` - Check training status
- `GET /api/ml/model-info/:companyId` - Get model information
- `GET /api/ml/validate-data/:companyId` - Validate company data

## **Database Schema:**

### **Company Models Collection (in Atlas)**
```javascript
{
  _id: ObjectId,
  company_id: String,  // From API parameter
  base_model_id: String,
  model_type: String,  // "finetuned"
  model_data: Binary,  // Serialized PyTorch model
  feature_columns: Array,
  metrics: Object,
  created_at: Date
}
```

## **Setup:**

1. **Install Dependencies**: `npm install`
2. **Start Backend**: `npm start` or `node server.js`
3. **Start ML Service**: `cd ml-service && python app.py`

## **Usage:**

### **1. Process Raw CSV**
```bash
POST /api/data/convert/{company_id}
# Upload CSV file, get processed nodes/edges/demand paths
```

### **2. Fine-tune Model**
```bash
POST /api/ml/fine-tune/{company_id}
Body: {
  "nodes": "path/to/nodes.csv",
  "edges": "path/to/edges.csv", 
  "demand": "path/to/demand.csv"
}
```

### **3. Make Predictions**
```bash
POST /api/ml/predict/{company_id}
Body: {
  "input_data": {...}
}
```

## **Why This is Perfect:**

1. **âœ… Simple** - No unnecessary complexity
2. **âœ… Direct** - Straight to ML operations
3. **âœ… Clean** - Minimal codebase
4. **âœ… Working** - Core functionality intact
5. **âœ… Scalable** - Easy to add features

## **What We Removed (Unnecessary):**

- âŒ Company model and validation
- âŒ Company status tracking
- âŒ Complex file management
- âŒ Unnecessary database operations

## **What We Kept (Essential):**

- âœ… File upload and processing
- âœ… ML service integration
- âœ… Model storage in Atlas
- âœ… Simple API endpoints

The system is now **clean, simple, and focused** on what actually matters - ML operations! ğŸ¯
